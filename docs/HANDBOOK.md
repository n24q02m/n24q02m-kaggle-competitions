# Kaggle Competitions Handbook

HÆ°á»›ng dáº«n toÃ n diá»‡n vá» quy trÃ¬nh thi Ä‘áº¥u Kaggle vÃ  sá»­ dá»¥ng workspace nÃ y.

## Má»¥c Lá»¥c

1. [Giá»›i Thiá»‡u Kaggle Competitions](#1-giá»›i-thiá»‡u-kaggle-competitions)
2. [Quy TrÃ¬nh Huáº¥n Luyá»‡n Model Chuáº©n](#2-quy-trÃ¬nh-huáº¥n-luyá»‡n-model-chuáº©n)
3. [Ká»¹ Thuáº­t NÃ¢ng Cao](#3-ká»¹-thuáº­t-nÃ¢ng-cao)
4. [Sá»­ Dá»¥ng Workspace](#4-sá»­-dá»¥ng-workspace)
5. [Best Practices Theo MÃ´i TrÆ°á»ng](#5-best-practices-theo-mÃ´i-trÆ°á»ng)
6. [Troubleshooting](#6-troubleshooting)

---

## 1. Giá»›i Thiá»‡u Kaggle Competitions

### 1.1 CÃ¡c Loáº¡i Cuá»™c Thi

**Getting Started (Nháº­p mÃ´n)**
- KhÃ´ng háº¿t háº¡n, dá»¯ liá»‡u sáº¡ch
- VD: Titanic, House Prices
- Má»¥c Ä‘Ã­ch: Há»c vÃ  thá»±c hÃ nh

**Playground Series**
- Diá»…n ra hÃ ng thÃ¡ng
- Dá»¯ liá»‡u synthetic, bÃ i toÃ¡n thá»±c táº¿
- Cá»™ng Ä‘á»“ng sÃ´i ná»•i, nhiá»u notebooks chia sáº»

**Featured Competitions**
- BÃ i toÃ¡n thá»±c táº¿ tá»« doanh nghiá»‡p
- CÃ³ giáº£i thÆ°á»Ÿng lá»›n
- YÃªu cáº§u ká»¹ nÄƒng cao

**Research Competitions**
- Táº­p trung nghiÃªn cá»©u khoa há»c
- VD: ARC Prize, AI Mathematical Olympiad

### 1.2 Lá»™ TrÃ¬nh Há»c Táº­p

```mermaid
graph LR
    A[Tuáº§n 1-2: Titanic + House Prices] --> B[Tuáº§n 3-4: Playground Series]
    B --> C[ThÃ¡ng 2-3: NLP Competitions]
    C --> D[DÃ i háº¡n: Featured Competitions]
```

**Giai Ä‘oáº¡n 1: Ná»n táº£ng (2 tuáº§n)**
- HoÃ n thÃ nh Titanic vÃ  House Prices
- Focus: XÃ¢y dá»±ng pipeline chuáº©n tá»« EDA â†’ Model â†’ Submission

**Giai Ä‘oáº¡n 2: Thá»±c hÃ nh (2 tuáº§n)**
- Tham gia Playground Series hiá»‡n táº¡i
- Äá»c notebooks top Ä‘á»ƒ há»c EDA vÃ  feature engineering

**Giai Ä‘oáº¡n 3: ChuyÃªn sÃ¢u (2-3 thÃ¡ng)**
- Chá»n 1 lÄ©nh vá»±c (NLP/CV/Tabular) Ä‘á»ƒ Ä‘Ã o sÃ¢u
- Thá»­ cÃ¡c ká»¹ thuáº­t nÃ¢ng cao

**Giai Ä‘oáº¡n 4: Thá»±c chiáº¿n**
- Tham gia Featured competitions
- Má»¥c tiÃªu top 10%

---

## 2. Quy TrÃ¬nh Huáº¥n Luyá»‡n Model Chuáº©n

### 2.1 Cáº¥u TrÃºc Notebook Chuáº©n

Má»™t notebook ML chuáº©n gá»“m 7 pháº§n:

#### **1. Khá»Ÿi Táº¡o & Cáº¥u HÃ¬nh**

```python
class CFG:
    seed = 42                    # Reproducibility
    n_folds = 5                  # Cross-validation
    target_col = 'label'
    model_name = 'xgboost'
    
# Seed everything
def seed_everything(seed):
    np.random.seed(seed)
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
```

**Táº¡i sao quan trá»ng:**
- `seed` Ä‘áº£m báº£o káº¿t quáº£ tÃ¡i táº¡o Ä‘Æ°á»£c
- Config táº­p trung giÃºp dá»… tune hyperparameters

#### **2. Tiá»‡n Ãch & HÃ m Há»— Trá»£**

```python
def reduce_mem_usage(df):
    """Giáº£m memory usage cá»§a DataFrame"""
    # Downcast int64 -> int32, float64 -> float32
    pass

def plot_feature_importance(model, features):
    """Váº½ feature importance"""
    pass
```

#### **3. EDA (Exploratory Data Analysis)**

```python
# Basic checks
print(df.info())
print(df.describe())

# Missing values
missing = df.isnull().sum()
plt.barh(missing[missing > 0].index, missing[missing > 0].values)

# Target distribution
df[target].value_counts().plot(kind='bar')

# Correlations
sns.heatmap(df.corr(), annot=True)
```

**Checklist EDA:**
- [ ] Kiá»ƒm tra shape, dtypes
- [ ] Missing values vÃ  outliers
- [ ] Target distribution (imbalanced?)
- [ ] Feature correlations
- [ ] Adversarial validation (train vs test distribution)

#### **4. Feature Engineering**

```python
# Example features
df['FamilySize'] = df['SibSp'] + df['Parch'] + 1
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.')
df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 60, 100])
```

**NguyÃªn táº¯c vÃ ng:**
- Domain knowledge > Complex features
- Báº¯t Ä‘áº§u Ä‘Æ¡n giáº£n, thÃªm dáº§n
- Validate má»—i feature thÃ´ng qua CV score

#### **5. Modeling Strategy**

```python
# Cross-validation split
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_folds=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    
    # Train model
    model.fit(X_train, y_train)
    
    # Validate
    preds = model.predict(X_val)
    score = metric(y_val, preds)
    print(f"Fold {fold}: {score}")
```

**Cross-Validation Strategies:**
- `StratifiedKFold`: PhÃ¢n loáº¡i, Ä‘áº£m báº£o tá»‰ lá»‡ classes
- `GroupKFold`: Data cÃ³ groups (VD: nhiá»u áº£nh cá»§a 1 ngÆ°á»i)
- `TimeSeriesSplit`: Dá»¯ liá»‡u chuá»—i thá»i gian

#### **6. Training Loop**

```python
oof_preds = np.zeros(len(train))  # Out-of-fold predictions
test_preds = np.zeros(len(test))

for fold in range(CFG.n_folds):
    # Train on fold
    model = train_model(fold)
    
    # OOF predictions
    oof_preds[val_idx] = model.predict(X_val)
    
    # Test predictions
    test_preds += model.predict(X_test) / CFG.n_folds
    
    # Save model
    save_model(model, f'model_fold{fold}.pkl')

# Overall CV score
cv_score = metric(y_train, oof_preds)
print(f"Overall CV: {cv_score}")
```

#### **7. Submission**

```python
submission = pd.DataFrame({
    'id': test_ids,
    'target': test_preds
})
submission.to_csv('submission.csv', index=False)
```

### 2.2 Quy TrÃ¬nh 3 Giai Äoáº¡n

**Giai Ä‘oáº¡n 1: Data-Centric (60% thá»i gian)**
- Hiá»ƒu dá»¯ liá»‡u
- LÃ m sáº¡ch
- Feature Engineering

**Giai Ä‘oáº¡n 2: Model-Centric (30% thá»i gian)**
- Chá»n CV strategy
- Chá»n baseline model
- Tune hyperparameters
- Train & validate

**Giai Ä‘oáº¡n 3: Result-Centric (10% thá»i gian)**
- Error analysis
- Post-processing
- Ensembling

---

## 3. Ká»¹ Thuáº­t NÃ¢ng Cao

### 3.1 Feature Engineering NÃ¢ng Cao

#### **Target Encoding**

```python
def target_encode(train, test, col, target, smoothing=10):
    # TÃ­nh mean target per category
    means = train.groupby(col)[target].mean()
    global_mean = train[target].mean()
    
    # Smoothing
    counts = train.groupby(col).size()
    smooth_means = (means * counts + global_mean * smoothing) / (counts + smoothing)
    
    # Apply
    train[f'{col}_target_enc'] = train[col].map(smooth_means)
    test[f'{col}_target_enc'] = test[col].map(smooth_means).fillna(global_mean)
```

**LÆ°u Ã½:** Target encoding dá»… overfit, pháº£i dÃ¹ng CV properly.

#### **Aggregation Features**

```python
# Group statistics
agg_features = data.groupby('user_id').agg({
    'purchase_amount': ['mean', 'std', 'min', 'max'],
    'purchase_count': 'sum'
}).reset_index()
```

#### **Lag & Rolling Features (Time Series)**

```python
# Lag features
data['sales_lag1'] = data.groupby('store_id')['sales'].shift(1)
data['sales_lag7'] = data.groupby('store_id')['sales'].shift(7)

# Rolling statistics
data['sales_rolling_mean_7d'] = data.groupby('store_id')['sales'].rolling(7).mean().values
```

### 3.2 Ensemble Techniques

#### **Simple Averaging**

```python
# Average predictions from multiple models
final_pred = (pred_xgb + pred_lgbm + pred_catboost) / 3
```

#### **Weighted Averaging**

```python
# TÃ¬m weights tá»‘i Æ°u
from scipy.optimize import minimize

def objective(weights):
    blend = weights[0]*pred1 + weights[1]*pred2 + weights[2]*pred3
    return -metric(y_true, blend)

result = minimize(objective, [0.33, 0.33, 0.34], bounds=[(0,1)]*3)
optimal_weights = result.x
```

#### **Stacking**

```python
# Level 1: Base models
base_models = [XGBoost(), LightGBM(), CatBoost()]
level1_train = np.zeros((len(train), len(base_models)))

for i, model in enumerate(base_models):
    oof_preds = cross_val_predict(model, X_train, y_train, cv=5)
    level1_train[:, i] = oof_preds

# Level 2: Meta model
meta_model = LogisticRegression()
meta_model.fit(level1_train, y_train)
```

### 3.3 Hyperparameter Tuning vá»›i Optuna

```python
import optuna

def objective(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000)
    }
    
    model = XGBClassifier(**params)
    score = cross_val_score(model, X, y, cv=5).mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
```

### 3.4 Ká»¹ Thuáº­t Xá»­ LÃ½ Dá»¯ Liá»‡u

#### **Adversarial Validation**

```python
# Gá»™p train vÃ  test
train['is_test'] = 0
test['is_test'] = 1
combined = pd.concat([train, test])

# Train classifier phÃ¢n biá»‡t train vs test
X = combined.drop(['target', 'is_test'], axis=1)
y = combined['is_test']

model = RandomForestClassifier()
auc = cross_val_score(model, X, y, scoring='roc_auc', cv=5).mean()

print(f"AUC: {auc}")
# Náº¿u AUC > 0.8: Train vÃ  Test khÃ¡c nhau nhiá»u
# Náº¿u AUC ~ 0.5: Train vÃ  Test giá»‘ng nhau
```

#### **Pseudo-Labeling**

```python
# Train model láº§n 1
model.fit(X_train, y_train)

# Predict test
test_probs = model.predict_proba(X_test)

# Láº¥y samples cÃ³ high confidence
confident_idx = (test_probs.max(axis=1) > 0.95)
pseudo_X = X_test[confident_idx]
pseudo_y = test_probs[confident_idx].argmax(axis=1)

# Gá»™p vÃ o train vÃ  train láº¡i
X_extended = pd.concat([X_train, pseudo_X])
y_extended = pd.concat([y_train, pd.Series(pseudo_y)])

model.fit(X_extended, y_extended)
```

---

## 4. Sá»­ Dá»¥ng Workspace

### 4.1 Setup Local

#### **BÆ°á»›c 1: Táº¡o Environment**

```bash
# Clone repo
git clone https://github.com/n24q02m/n24q02m-kaggle-competitions.git
cd n24q02m-kaggle-competitions

# Táº¡o conda env
conda env create -f environment.yml

# Activate
conda activate kaggle-competitions
```

#### **BÆ°á»›c 2: Setup Kaggle API**

```bash
# Download credentials tá»« Kaggle â†’ Account â†’ API â†’ Create New Token
# Di chuyá»ƒn kaggle.json vÃ o ~/.kaggle/
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json
```

#### **BÆ°á»›c 3: Download Competition Data**

```bash
# Download Titanic data
kaggle competitions download -c titanic -p competitions/titanic/data

# Unzip
cd competitions/titanic/data
unzip titanic.zip
rm titanic.zip
```

#### **BÆ°á»›c 4: Má»Ÿ Notebook trong VS Code**

```bash
# Má»Ÿ VS Code
code .

# Trong VS Code:
# 1. Chá»n kernel: kaggle-competitions
# 2. Má»Ÿ file: competitions/titanic/notebooks/solution.ipynb
# 3. Báº¯t Ä‘áº§u code!
```

### 4.2 Setup Google Colab

**BÆ°á»›c 1: Upload Notebook**
- Upload `solution.ipynb` lÃªn Colab
- Hoáº·c: File â†’ Upload notebook â†’ GitHub URL

**BÆ°á»›c 2: Cháº¡y Bootstrap Cell**
- Cell Ä‘áº§u tiÃªn sáº½ tá»± Ä‘á»™ng:
  - Mount Google Drive (náº¿u cáº§n)
  - Download `setup_env.py` tá»« GitHub
  - CÃ i requirements

**BÆ°á»›c 3: Upload Data**
```python
from google.colab import files
uploaded = files.upload()  # Upload train.csv, test.csv
```

Hoáº·c mount Drive vÃ  dÃ¹ng data tá»« Drive.

### 4.3 Setup Kaggle Kernels

**BÆ°á»›c 1: Create Notebook**
- Kaggle â†’ Competitions â†’ Titanic â†’ Code â†’ New Notebook

**BÆ°á»›c 2: Copy Bootstrap Cell**
- Copy bootstrap cell tá»« template
- Thay Ä‘á»•i paths Ä‘á»ƒ dÃ¹ng `/kaggle/input/titanic`

**BÆ°á»›c 3: Code!**
- Data Ä‘Ã£ sáºµn táº¡i `/kaggle/input/titanic/`
- KhÃ´ng cáº§n download

### 4.4 Workflow Báº¯t Äáº§u Cuá»™c Thi Má»›i

```bash
# 1. Táº¡o structure
mkdir -p competitions/new-competition/{data,notebooks,models,submissions}

# 2. Copy template
cp competitions/titanic/notebooks/solution.ipynb \\
   competitions/new-competition/notebooks/

# 3. Download data
kaggle competitions download -c new-competition \\
  -p competitions/new-competition/data

# 4. Update notebook paths vÃ  code!
```

---

## 5. Best Practices Theo MÃ´i TrÆ°á»ng

### 5.1 Local Development

**Æ¯u Ä‘iá»ƒm:**
- Full control
- DÃ¹ng Ä‘Æ°á»£c GPU cÃ¡ nhÃ¢n
- KhÃ´ng bá»‹ giá»›i háº¡n thá»i gian
- Code versioning dá»… dÃ ng

**Best Practices:**
- âœ… LuÃ´n dÃ¹ng conda environment riÃªng
- âœ… Commit code thÆ°á»ng xuyÃªn (Git)
- âœ… Track experiments (MLflow, Weights & Biases)
- âœ… Save models vá»›i versioning (`model_v1_cv0.85.pkl`)

**Checklist:**
- [ ] Activated conda env
- [ ] Git commit sau má»—i experiment
- [ ] Model vÃ  results Ä‘Æ°á»£c save
- [ ] Notebooks cÃ³ markdown giáº£i thÃ­ch

### 5.2 Google Colab

**Æ¯u Ä‘iá»ƒm:**
- Miá»…n phÃ­ GPU/TPU
- KhÃ´ng cáº§n setup mÃ¡y
- Dá»… chia sáº»

**NhÆ°á»£c Ä‘iá»ƒm:**
- Timeout sau 12h
- Máº¥t data khi disconnect
- RAM giá»›i háº¡n

**Best Practices:**
- âœ… LÆ°u checkpoints thÆ°á»ng xuyÃªn vÃ o Drive
- âœ… DÃ¹ng `%%time` Ä‘á»ƒ track thá»i gian cells
- âœ… Clear output khÃ´ng cáº§n thiáº¿t (tiáº¿t kiá»‡m RAM)
- âš ï¸ Mount Drive ngay Ä‘áº§u session

**Tricks:**
```python
# Auto-reconnect (chá»‘ng disconnect)
function ClickConnect(){
  console.log("Working");
  document.querySelector("colab-toolbar-button#connect").click()
}
setInterval(ClickConnect,60000)
```

### 5.3 Kaggle Kernels

**Æ¯u Ä‘iá»ƒm:**
- Data cÃ³ sáºµn
- Share code dá»… dÃ ng
- Äiá»ƒm cá»™ng cho leaderboard

**NhÆ°á»£c Ä‘iá»ƒm:**
- 9h/week GPU quota
- Internet off trong competition

**Best Practices:**
- âœ… Develop locally, test trÃªn Kaggle
- âœ… DÃ¹ng Kaggle Datasets cho external data
- âœ… Enable GPU chá»‰ khi cáº§n
- âœ… Comment code rÃµ rÃ ng (cho community)

---

## 6. Troubleshooting

### 6.1 Common Issues

#### **"Module not found" trÃªn Colab/Kaggle**

```python
# Option 1: Install thiáº¿u package
!pip install -q package-name

# Option 2: Check import path
import sys
print(sys.path)
```

#### **"Out of Memory" error**

```python
# Giáº£m batch size
CFG.batch_size = 16  # thay vÃ¬ 32

# Clear memory
import gc
del large_variable
gc.collect()

# DÃ¹ng dtype nháº¹ hÆ¡n
df = df.astype('float32')  # thay vÃ¬ float64
```

#### **CV score tá»‘t nhÆ°ng LB score kÃ©m**

**NguyÃªn nhÃ¢n:**
- Overfitting validation set
- Train/Test distribution khÃ¡c nhau
- Data leakage

**Giáº£i phÃ¡p:**
- Kiá»ƒm tra Adversarial Validation
- ThÃªm regularization
- Kiá»ƒm tra láº¡i feature engineering

#### **Bootstrap cell failed trÃªn Colab**

```python
# Check internet connection
!ping -c 3 raw.githubusercontent.com

# Check GitHub URL
CORE_URL = f"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/{BRANCH}/core"
print(CORE_URL)

# Manual download
!wget {CORE_URL}/setup_env.py
```

### 6.2 Performance Tips

#### **Speed up pandas operations**

```python
# DÃ¹ng category dtype cho categorical cols
df['category_col'] = df['category_col'].astype('category')

# Vectorize thay vÃ¬ apply
df['new_col'] = df['col1'] + df['col2']  # Fast
# df['new_col'] = df.apply(lambda x: x['col1'] + x['col2'], axis=1)  # Slow
```

#### **Speed up model training**

```python
# XGBoost: early stopping
model.fit(X_train, y_train,
          eval_set=[(X_val, y_val)],
          early_stopping_rounds=50,
          verbose=False)

# DÃ¹ng GPU
XGBClassifier(tree_method='gpu_hist')
```

### 6.3 Debugging Tips

```python
# Print shapes thÆ°á»ng xuyÃªn
print(f"Train: {X_train.shape}, Test: {X_test.shape}")

# Check for NaN
assert not df.isnull().any().any(), "Found NaN values!"

# Validate predictions
assert len(predictions) == len(test), "Prediction length mismatch!"
assert predictions.min() >= 0 and predictions.max() <= 1, "Invalid probability!"
```

---

## TÃ i NguyÃªn Tham Kháº£o

### Há»c Táº­p
- [Kaggle Learn](https://www.kaggle.com/learn)
- [Fast.ai Courses](https://www.fast.ai/)
- [Coursera ML Specialization](https://www.coursera.org/specializations/machine-learning-introduction)

### Tools
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Scikit-learn Documentation](https://scikit-learn.org/)

### Community
- [Kaggle Forums](https://www.kaggle.com/discussion)
- [Reddit r/MachineLearning](https://www.reddit.com/r/MachineLearning/)

---

**Happy Kaggling! ðŸš€**
