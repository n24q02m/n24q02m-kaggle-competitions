{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9a3a63",
   "metadata": {},
   "source": [
    "# ONNX Model Converter "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564f40e",
   "metadata": {},
   "source": [
    "## 1. Cấu Hình Model\n",
    "\n",
    "Thay đổi các giá trị bên dưới để cấu hình model cần chuyển đổi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90318bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CẤU HÌNH - THAY ĐỔI CÁC GIÁ TRỊ NÀY\n",
    "# ============================================================\n",
    "\n",
    "# Model HuggingFace cần chuyển đổi\n",
    "MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "\n",
    "# Task của model (auto = tự động detect, hoặc chọn thủ công)\n",
    "# Các giá trị: \"auto\", \"feature-extraction\", \"text-classification\", \"text-generation\"\n",
    "TASK = \"auto\"\n",
    "\n",
    "# Tên output (để trống = tự động tạo từ MODEL_ID)\n",
    "OUTPUT_NAME = \"\"\n",
    "\n",
    "# HuggingFace username của bạn\n",
    "HF_USERNAME = \"n24q02m\"\n",
    "\n",
    "# Bật/tắt quantization INT8\n",
    "ENABLE_QUANTIZATION = True\n",
    "\n",
    "# ============================================================\n",
    "# KHÔNG CẦN THAY ĐỔI PHẦN DƯỚI\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fc512",
   "metadata": {},
   "source": [
    "## 2. Thiết Lập Môi Trường\n",
    "\n",
    "Tự động phát hiện và cấu hình môi trường (local/colab/kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324cb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import subprocess\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Phát hiện môi trường\n",
    "def detect_env():\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        return \"colab\"\n",
    "    elif \"kaggle_web_client\" in sys.modules or os.path.exists(\"/kaggle\"):\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = detect_env()\n",
    "print(f\"Môi trường: {ENV.upper()}\")\n",
    "\n",
    "# Thiết lập thư mục output\n",
    "if ENV == \"colab\":\n",
    "    OUTPUT_DIR = Path(\"/content/models\")\n",
    "elif ENV == \"kaggle\":\n",
    "    OUTPUT_DIR = Path(\"/kaggle/working/models\")\n",
    "else:\n",
    "    OUTPUT_DIR = Path.cwd() / \"models\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Thư mục output: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131e883",
   "metadata": {},
   "source": [
    "## 3. Import Thư Viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from optimum.onnxruntime import (\n",
    "    ORTModelForFeatureExtraction,\n",
    "    ORTModelForSequenceClassification,\n",
    ")\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Giải phóng bộ nhớ RAM và GPU\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"ONNX: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime: {ort.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf68145",
   "metadata": {},
   "source": [
    "## 4. Xử Lý Cấu Hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748ab21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tự động detect task từ model_id\n",
    "def detect_task(model_id):\n",
    "    \"\"\"Tự động detect task dựa trên tên model\"\"\"\n",
    "    model_lower = model_id.lower()\n",
    "    if \"embedding\" in model_lower:\n",
    "        return \"feature-extraction\"\n",
    "    elif \"reranker\" in model_lower or \"ranker\" in model_lower:\n",
    "        return \"text-classification\"\n",
    "    elif \"classifier\" in model_lower or \"classification\" in model_lower:\n",
    "        return \"text-classification\"\n",
    "    else:\n",
    "        # Thử đọc config để detect\n",
    "        try:\n",
    "            config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "            if hasattr(config, \"num_labels\") and config.num_labels > 0:\n",
    "                return \"text-classification\"\n",
    "        except:\n",
    "            pass\n",
    "        return \"feature-extraction\"\n",
    "\n",
    "\n",
    "# Xử lý cấu hình\n",
    "if TASK == \"auto\":\n",
    "    TASK = detect_task(MODEL_ID)\n",
    "    print(f\"Task tự động detect: {TASK}\")\n",
    "\n",
    "if not OUTPUT_NAME:\n",
    "    # Tạo tên từ MODEL_ID\n",
    "    OUTPUT_NAME = MODEL_ID.split(\"/\")[-1] + \"-ONNX\"\n",
    "\n",
    "# Tạo thư mục cho model\n",
    "MODEL_OUTPUT_DIR = OUTPUT_DIR / OUTPUT_NAME.lower().replace(\" \", \"-\")\n",
    "FP32_DIR = MODEL_OUTPUT_DIR / \"fp32\"\n",
    "INT8_DIR = MODEL_OUTPUT_DIR / \"int8\"\n",
    "\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FP32_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if ENABLE_QUANTIZATION:\n",
    "    INT8_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Chọn ORT class phù hợp\n",
    "if TASK == \"feature-extraction\":\n",
    "    ORTModelClass = ORTModelForFeatureExtraction\n",
    "elif TASK == \"text-classification\":\n",
    "    ORTModelClass = ORTModelForSequenceClassification\n",
    "else:\n",
    "    raise ValueError(f\"Task không được hỗ trợ: {TASK}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"CẤU HÌNH CUỐI CÙNG\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model ID: {MODEL_ID}\")\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Output Name: {OUTPUT_NAME}\")\n",
    "print(f\"Output Dir: {MODEL_OUTPUT_DIR}\")\n",
    "print(f\"Quantization: {ENABLE_QUANTIZATION}\")\n",
    "print(f\"HF Username: {HF_USERNAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695007cd",
   "metadata": {},
   "source": [
    "## 5. Export Model sang ONNX (FP32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"EXPORT ONNX (FP32)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nĐang tải và export model: {MODEL_ID}\")\n",
    "print(\"Quá trình này có thể mất vài phút...\")\n",
    "\n",
    "export_success = False\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "    # Export sang ONNX\n",
    "    ort_model = ORTModelClass.from_pretrained(\n",
    "        MODEL_ID, export=True, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Lưu model và tokenizer\n",
    "    ort_model.save_pretrained(FP32_DIR)\n",
    "    tokenizer.save_pretrained(FP32_DIR)\n",
    "\n",
    "    print(f\"\\nModel đã được export tại: {FP32_DIR}\")\n",
    "\n",
    "    # Hiển thị files\n",
    "    print(\"\\nDanh sách files:\")\n",
    "    total_size = 0\n",
    "    for f in FP32_DIR.iterdir():\n",
    "        if f.is_file():\n",
    "            size_mb = f.stat().st_size / (1024**2)\n",
    "            total_size += size_mb\n",
    "            print(f\"  - {f.name} ({size_mb:.2f} MB)\")\n",
    "    print(f\"  Tổng: {total_size:.2f} MB\")\n",
    "\n",
    "    # Giải phóng bộ nhớ\n",
    "    del ort_model, tokenizer\n",
    "    clear_memory()\n",
    "    export_success = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nLỗi khi export: {e}\")\n",
    "    print(\"\\nThử phương pháp CLI...\")\n",
    "\n",
    "    try:\n",
    "        cmd = f'optimum-cli export onnx --model \"{MODEL_ID}\" --task {TASK} --trust-remote-code \"{FP32_DIR}\"'\n",
    "        result = subprocess.run(\n",
    "            cmd, shell=True, capture_output=True, text=True, timeout=600\n",
    "        )\n",
    "\n",
    "        if list(FP32_DIR.glob(\"*.onnx\")):\n",
    "            print(\"Export thành công qua CLI!\")\n",
    "            export_success = True\n",
    "        else:\n",
    "            print(f\"CLI thất bại: {result.stderr}\")\n",
    "    except Exception as e2:\n",
    "        print(f\"CLI cũng thất bại: {e2}\")\n",
    "\n",
    "    clear_memory()\n",
    "\n",
    "if export_success:\n",
    "    print(\"\\nExport FP32: THÀNH CÔNG\")\n",
    "else:\n",
    "    print(\"\\nExport FP32: THẤT BẠI\")\n",
    "    raise Exception(\"Không thể export model sang ONNX\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd45afcf",
   "metadata": {},
   "source": [
    "## 6. Quantization (INT8)\n",
    "\n",
    "Áp dụng Dynamic Quantization để giảm kích thước model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_success = False\n",
    "\n",
    "if ENABLE_QUANTIZATION:\n",
    "    print(\"=\" * 50)\n",
    "    print(\"QUANTIZATION (INT8)\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\nĐang quantize model...\")\n",
    "    print(\"(Có thể mất vài phút)\")\n",
    "\n",
    "    # Phương pháp: Sử dụng subprocess.Popen để tránh timeout\n",
    "    cmd = f'optimum-cli onnxruntime quantize --onnx_model \"{FP32_DIR}\" --avx512_vnni -o \"{INT8_DIR}\"'\n",
    "\n",
    "    try:\n",
    "        # Chạy với Popen để có thể monitor\n",
    "        process = subprocess.Popen(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "        )\n",
    "\n",
    "        # Đọc output real-time\n",
    "        output_lines = []\n",
    "        for line in process.stdout:\n",
    "            print(line, end=\"\")\n",
    "            output_lines.append(line)\n",
    "\n",
    "        process.wait(timeout=600)  # 10 phút timeout\n",
    "\n",
    "        # Đợi file được ghi xong\n",
    "        time.sleep(3)\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        process.kill()\n",
    "        print(\"\\nTimeout - process đã bị kill\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nLỗi: {e}\")\n",
    "\n",
    "    # Kiểm tra kết quả\n",
    "    int8_files = list(INT8_DIR.glob(\"*.onnx\"))\n",
    "\n",
    "    if int8_files:\n",
    "        print(\"\\nQuantization: THÀNH CÔNG\")\n",
    "        quantize_success = True\n",
    "\n",
    "        # Copy tokenizer files\n",
    "        for f in FP32_DIR.iterdir():\n",
    "            if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "                if f.is_file() and not (INT8_DIR / f.name).exists():\n",
    "                    shutil.copy(f, INT8_DIR / f.name)\n",
    "\n",
    "        # So sánh kích thước\n",
    "        fp32_size = sum(f.stat().st_size for f in FP32_DIR.glob(\"*.onnx\")) / (1024**2)\n",
    "        int8_size = sum(f.stat().st_size for f in INT8_DIR.glob(\"*.onnx\")) / (1024**2)\n",
    "\n",
    "        print(f\"\\nSo sánh kích thước:\")\n",
    "        print(f\"  FP32: {fp32_size:.2f} MB\")\n",
    "        print(f\"  INT8: {int8_size:.2f} MB\")\n",
    "        if fp32_size > 0:\n",
    "            print(f\"  Giảm: {(1 - int8_size / fp32_size) * 100:.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nQuantization: THẤT BẠI\")\n",
    "        print(\"Chỉ có model FP32 khả dụng.\")\n",
    "\n",
    "    clear_memory()\n",
    "else:\n",
    "    print(\"Quantization đã tắt. Bỏ qua...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf36cb61",
   "metadata": {},
   "source": [
    "## 7. Kiểm Tra Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f59ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"KIỂM TRA MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "\n",
    "def test_model(model_path, model_name, task):\n",
    "    \"\"\"Test model ONNX\"\"\"\n",
    "    try:\n",
    "        if not list(Path(model_path).glob(\"*.onnx\")):\n",
    "            print(f\"{model_name}: Không tìm thấy file ONNX\")\n",
    "            return False\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        if task == \"feature-extraction\":\n",
    "            model = ORTModelForFeatureExtraction.from_pretrained(model_path)\n",
    "            test_text = \"This is a test sentence.\"\n",
    "            inputs = tokenizer(\n",
    "                test_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "\n",
    "            # Thêm position_ids nếu cần (cho Qwen models)\n",
    "            if \"position_ids\" not in inputs:\n",
    "                inputs[\"position_ids\"] = torch.arange(\n",
    "                    inputs[\"input_ids\"].shape[1]\n",
    "                ).unsqueeze(0)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            print(f\"{model_name}: OK (output shape: {outputs.last_hidden_state.shape})\")\n",
    "\n",
    "        elif task == \"text-classification\":\n",
    "            model = ORTModelForSequenceClassification.from_pretrained(model_path)\n",
    "            query = \"What is machine learning?\"\n",
    "            doc = \"Machine learning is a branch of AI.\"\n",
    "            inputs = tokenizer(\n",
    "                query, doc, return_tensors=\"pt\", padding=True, truncation=True\n",
    "            )\n",
    "            outputs = model(**inputs)\n",
    "            print(f\"{model_name}: OK (logits: {outputs.logits.shape})\")\n",
    "\n",
    "        del model, tokenizer\n",
    "        clear_memory()\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name}: LỖI - {e}\")\n",
    "        clear_memory()\n",
    "        return False\n",
    "\n",
    "\n",
    "# Test FP32\n",
    "print(\"\\n--- Test FP32 ---\")\n",
    "fp32_ok = test_model(FP32_DIR, \"FP32\", TASK)\n",
    "\n",
    "# Test INT8\n",
    "if ENABLE_QUANTIZATION and quantize_success:\n",
    "    print(\"\\n--- Test INT8 ---\")\n",
    "    int8_ok = test_model(INT8_DIR, \"INT8\", TASK)\n",
    "else:\n",
    "    int8_ok = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2bbd50",
   "metadata": {},
   "source": [
    "## 8. Tạo Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be101be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"TẠO MODEL CARD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Xác định model type\n",
    "model_type = \"embedding\" if TASK == \"feature-extraction\" else \"reranker\"\n",
    "ort_class = (\n",
    "    \"FeatureExtraction\" if TASK == \"feature-extraction\" else \"SequenceClassification\"\n",
    ")\n",
    "\n",
    "# Xác định versions\n",
    "versions = [\"FP32\"]\n",
    "if ENABLE_QUANTIZATION and quantize_success:\n",
    "    versions.append(\"INT8 Dynamic\")\n",
    "\n",
    "model_card = f'''---\n",
    "license: apache-2.0\n",
    "tags:\n",
    "  - onnx\n",
    "  - optimum\n",
    "  - {model_type}\n",
    "base_model: {MODEL_ID}\n",
    "library_name: optimum\n",
    "pipeline_tag: {TASK}\n",
    "---\n",
    "\n",
    "# {OUTPUT_NAME}\n",
    "\n",
    "ONNX version of [{MODEL_ID}](https://huggingface.co/{MODEL_ID})\n",
    "\n",
    "## Model Information\n",
    "\n",
    "| Property | Value |\n",
    "|----------|-------|\n",
    "| Source Model | [{MODEL_ID}](https://huggingface.co/{MODEL_ID}) |\n",
    "| Format | ONNX |\n",
    "| Versions | {\" + \".join(versions)} |\n",
    "| Task | {TASK} |\n",
    "\n",
    "## Available Versions\n",
    "\n",
    "- `fp32/`: Full precision (FP32)\n",
    "{\"- `int8/`: Quantized INT8 (dynamic quantization)\" if ENABLE_QUANTIZATION and quantize_success else \"\"}\n",
    "\n",
    "## Usage\n",
    "\n",
    "### With Optimum\n",
    "\n",
    "```python\n",
    "from optimum.onnxruntime import ORTModelFor{ort_class}\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = ORTModelFor{ort_class}.from_pretrained(\n",
    "    \"{HF_USERNAME}/{OUTPUT_NAME}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"{HF_USERNAME}/{OUTPUT_NAME}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Apache 2.0 (following the original model license)\n",
    "\n",
    "## Credits\n",
    "\n",
    "- Original model: [{MODEL_ID}](https://huggingface.co/{MODEL_ID})\n",
    "- ONNX conversion: [Optimum](https://huggingface.co/docs/optimum)\n",
    "'''\n",
    "\n",
    "# Lưu Model Card\n",
    "readme_path = MODEL_OUTPUT_DIR / \"README.md\"\n",
    "readme_path.write_text(model_card, encoding=\"utf-8\")\n",
    "print(f\"Đã tạo: {readme_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8263e4",
   "metadata": {},
   "source": [
    "## 9. Hướng Dẫn Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c87f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"HƯỚNG DẪN UPLOAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\"\"\n",
    "=== CÁCH 1: HuggingFace CLI ===\n",
    "\n",
    "# Đăng nhập\n",
    "huggingface-cli login\n",
    "\n",
    "# Tạo repo\n",
    "huggingface-cli repo create {OUTPUT_NAME} --type model\n",
    "\n",
    "# Upload\n",
    "huggingface-cli upload {HF_USERNAME}/{OUTPUT_NAME} \"{MODEL_OUTPUT_DIR}\" .\n",
    "\n",
    "=== CÁCH 2: Web UI ===\n",
    "\n",
    "1. Truy cập: https://huggingface.co/new\n",
    "2. Tạo repo: {OUTPUT_NAME}\n",
    "3. Upload files từ: {MODEL_OUTPUT_DIR}\n",
    "\n",
    "=== CẤU TRÚC FILES ===\n",
    "\"\"\")\n",
    "\n",
    "# Hiển thị cấu trúc\n",
    "for item in sorted(MODEL_OUTPUT_DIR.rglob(\"*\")):\n",
    "    rel_path = item.relative_to(MODEL_OUTPUT_DIR)\n",
    "    indent = \"  \" * (len(rel_path.parts) - 1)\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"{indent}{item.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{indent}{item.name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cd60e",
   "metadata": {},
   "source": [
    "## 10. Tổng Kết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de8c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"TỔNG KẾT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nModel: {MODEL_ID}\")\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Output: {MODEL_OUTPUT_DIR}\")\n",
    "\n",
    "\n",
    "# Kích thước\n",
    "def get_folder_size(folder):\n",
    "    return sum(f.stat().st_size for f in Path(folder).rglob(\"*\") if f.is_file()) / (\n",
    "        1024**2\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"\\nKích thước:\")\n",
    "try:\n",
    "    total_size = get_folder_size(MODEL_OUTPUT_DIR)\n",
    "    print(f\"  Tổng: {total_size:.2f} MB\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\nKết quả:\")\n",
    "print(f\"  FP32: {'OK' if fp32_ok else 'LỖI'}\")\n",
    "if ENABLE_QUANTIZATION:\n",
    "    print(f\"  INT8: {'OK' if quantize_success and int8_ok else 'KHÔNG KHẢ DỤNG'}\")\n",
    "\n",
    "print(f\"\\nURL sau khi upload:\")\n",
    "print(f\"  https://huggingface.co/{HF_USERNAME}/{OUTPUT_NAME}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HOÀN THÀNH!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
