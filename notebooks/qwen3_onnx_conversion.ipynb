{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf467f47",
   "metadata": {},
   "source": [
    "# Qwen3 ONNX Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131875a",
   "metadata": {},
   "source": [
    "## 1. Bootstrap - Thiết Lập Môi Trường\n",
    "\n",
    "Cell này tự động phát hiện và cấu hình môi trường (local/colab/kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef18fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BOOTSTRAP CELL - UNIVERSAL SETUP ===\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Cấu hình GitHub\n",
    "GITHUB_USER = \"n24q02m\"\n",
    "REPO_NAME = \"n24q02m-kaggle-competitions\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "\n",
    "# Phát hiện môi trường\n",
    "def detect_env():\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        return \"colab\"\n",
    "    elif \"kaggle_web_client\" in sys.modules or os.path.exists(\"/kaggle\"):\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = detect_env()\n",
    "print(f\"Phát hiện môi trường: {ENV.upper()}\")\n",
    "\n",
    "# Thiết lập theo môi trường\n",
    "if ENV == \"local\":\n",
    "    # Local: Import trực tiếp từ repo\n",
    "    repo_root = Path.cwd().parent\n",
    "    if str(repo_root) not in sys.path:\n",
    "        sys.path.insert(0, str(repo_root))\n",
    "\n",
    "    from core import setup_env\n",
    "\n",
    "    env = setup_env.setup()\n",
    "\n",
    "else:\n",
    "    # Cloud: Tải setup_env.py từ GitHub\n",
    "    import requests\n",
    "    import subprocess\n",
    "\n",
    "    CORE_URL = (\n",
    "        f\"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/{BRANCH}/core\"\n",
    "    )\n",
    "\n",
    "    # Tải setup_env.py\n",
    "    print(\"Đang tải setup_env.py...\")\n",
    "    response = requests.get(f\"{CORE_URL}/setup_env.py\")\n",
    "    with open(\"setup_env.py\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    # Import và thiết lập\n",
    "    import setup_env\n",
    "\n",
    "    env = setup_env.setup(GITHUB_USER, REPO_NAME)\n",
    "\n",
    "# Hiển thị thông tin môi trường\n",
    "env.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c36cd66",
   "metadata": {},
   "source": [
    "## 2. Cấu Hình\n",
    "\n",
    "Cấu hình các model và đường dẫn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54aedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # Model nguồn từ HuggingFace\n",
    "    embedding_model_id = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    reranker_model_id = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "    # HuggingFace username (thay đổi theo username của bạn)\n",
    "    hf_username = \"n24q02m\"\n",
    "\n",
    "    # Tên repo đích trên HuggingFace Hub\n",
    "    embedding_repo_name = \"Qwen3-Embedding-0.6B-ONNX\"\n",
    "    reranker_repo_name = \"Qwen3-Reranker-0.6B-ONNX\"\n",
    "\n",
    "    # Thư mục output\n",
    "    output_dir = Path.cwd() / \"models\"\n",
    "    embedding_output_dir = output_dir / \"qwen3-embedding-onnx\"\n",
    "    reranker_output_dir = output_dir / \"qwen3-reranker-onnx\"\n",
    "\n",
    "    # Cài đặt ONNX Export\n",
    "    opset_version = 14\n",
    "\n",
    "    # Cài đặt Quantization\n",
    "    quantize = True\n",
    "    quantization_type = \"dynamic\"  # dynamic hoặc static\n",
    "\n",
    "\n",
    "# Tạo thư mục output\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "CFG.embedding_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "CFG.reranker_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Cấu hình:\")\n",
    "print(f\"  - Embedding Model: {CFG.embedding_model_id}\")\n",
    "print(f\"  - Reranker Model: {CFG.reranker_model_id}\")\n",
    "print(f\"  - Thư mục Output: {CFG.output_dir}\")\n",
    "print(f\"  - Quantization: {CFG.quantize} ({CFG.quantization_type})\")\n",
    "print(f\"  - HuggingFace Username: {CFG.hf_username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c40b1b",
   "metadata": {},
   "source": [
    "## 3. Cài Đặt Dependencies\n",
    "\n",
    "Cài đặt các thư viện cần thiết cho việc chuyển đổi ONNX và quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15114e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt các packages cần thiết\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"optimum[onnxruntime]\",\n",
    "    \"onnx\",\n",
    "    \"onnxruntime\",\n",
    "    \"transformers\",\n",
    "    \"torch\",\n",
    "    \"sentence-transformers\",\n",
    "]\n",
    "\n",
    "print(\"Đang cài đặt dependencies...\")\n",
    "for package in packages:\n",
    "    print(f\"  Đang cài đặt {package}...\")\n",
    "    subprocess.check_call(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "    )\n",
    "\n",
    "print(\"Cài đặt dependencies hoàn tất!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a205df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "from optimum.onnxruntime import (\n",
    "    ORTModelForFeatureExtraction,\n",
    "    ORTModelForSequenceClassification,\n",
    ")\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9004fd3",
   "metadata": {},
   "source": [
    "## 4. Tải Xuống và Export Embedding Model sang ONNX\n",
    "\n",
    "Tải xuống Qwen3-Embedding-0.6B từ HuggingFace và chuyển đổi sang định dạng ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b930e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDING MODEL - CHUYỂN ĐỔI ONNX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nĐang tải model: {CFG.embedding_model_id}\")\n",
    "print(\"Quá trình này có thể mất vài phút...\")\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CFG.embedding_model_id, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Export sang ONNX sử dụng optimum\n",
    "    embedding_ort_model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "        CFG.embedding_model_id, export=True, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Lưu model và tokenizer\n",
    "    embedding_onnx_path = CFG.embedding_output_dir / \"fp32\"\n",
    "    embedding_onnx_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    embedding_ort_model.save_pretrained(embedding_onnx_path)\n",
    "    embedding_tokenizer.save_pretrained(embedding_onnx_path)\n",
    "\n",
    "    print(f\"\\nEmbedding model đã được export tại: {embedding_onnx_path}\")\n",
    "    print(\"Danh sách files:\")\n",
    "    for f in embedding_onnx_path.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024**2) if f.is_file() else 0\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"  - {f.name}/\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi export embedding model: {e}\")\n",
    "    print(\"\\nThử phương pháp thay thế với optimum-cli...\")\n",
    "    !optimum-cli export onnx --model {CFG.embedding_model_id} --task feature-extraction --trust-remote-code {CFG.embedding_output_dir}/fp32/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f749ea8",
   "metadata": {},
   "source": [
    "## 5. Quantize Embedding Model (INT8)\n",
    "\n",
    "Áp dụng Dynamic Quantization để giảm kích thước model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.quantize:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EMBEDDING MODEL - QUANTIZATION (INT8)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    embedding_fp32_path = CFG.embedding_output_dir / \"fp32\"\n",
    "    embedding_int8_path = CFG.embedding_output_dir / \"int8\"\n",
    "    embedding_int8_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Tải ONNX model để quantize\n",
    "        quantizer = ORTQuantizer.from_pretrained(embedding_fp32_path)\n",
    "\n",
    "        # Cấu hình quantization - dynamic quantization\n",
    "        qconfig = AutoQuantizationConfig.avx512_vnni(\n",
    "            is_static=False,  # Dynamic quantization\n",
    "            per_channel=False,\n",
    "        )\n",
    "\n",
    "        # Áp dụng quantization\n",
    "        quantizer.quantize(save_dir=embedding_int8_path, quantization_config=qconfig)\n",
    "\n",
    "        # Copy các file tokenizer\n",
    "        for f in embedding_fp32_path.iterdir():\n",
    "            if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "                if f.is_file():\n",
    "                    shutil.copy(f, embedding_int8_path / f.name)\n",
    "\n",
    "        print(f\"\\nModel đã quantize được lưu tại: {embedding_int8_path}\")\n",
    "\n",
    "        # So sánh kích thước\n",
    "        fp32_size = sum(\n",
    "            f.stat().st_size for f in embedding_fp32_path.glob(\"*.onnx\")\n",
    "        ) / (1024**2)\n",
    "        int8_size = sum(\n",
    "            f.stat().st_size for f in embedding_int8_path.glob(\"*.onnx\")\n",
    "        ) / (1024**2)\n",
    "\n",
    "        print(f\"\\nSo sánh kích thước Model:\")\n",
    "        print(f\"  FP32: {fp32_size:.2f} MB\")\n",
    "        print(f\"  INT8: {int8_size:.2f} MB\")\n",
    "        print(f\"  Giảm: {(1 - int8_size / fp32_size) * 100:.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi quantize embedding model: {e}\")\n",
    "        print(\"Bỏ qua quantization cho embedding model.\")\n",
    "else:\n",
    "    print(\"Quantization đã tắt. Bỏ qua...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1965181",
   "metadata": {},
   "source": [
    "## 6. Tải Xuống và Export Reranker Model sang ONNX\n",
    "\n",
    "Tải xuống Qwen3-Reranker-0.6B từ HuggingFace và chuyển đổi sang định dạng ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1819b29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RERANKER MODEL - CHUYỂN ĐỔI ONNX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nĐang tải model: {CFG.reranker_model_id}\")\n",
    "print(\"Quá trình này có thể mất vài phút...\")\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer\n",
    "    reranker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CFG.reranker_model_id, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Export sang ONNX - Reranker là cross-encoder, sử dụng SequenceClassification\n",
    "    reranker_ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        CFG.reranker_model_id, export=True, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Lưu model và tokenizer\n",
    "    reranker_onnx_path = CFG.reranker_output_dir / \"fp32\"\n",
    "    reranker_onnx_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    reranker_ort_model.save_pretrained(reranker_onnx_path)\n",
    "    reranker_tokenizer.save_pretrained(reranker_onnx_path)\n",
    "\n",
    "    print(f\"\\nReranker model đã được export tại: {reranker_onnx_path}\")\n",
    "    print(\"Danh sách files:\")\n",
    "    for f in reranker_onnx_path.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024**2) if f.is_file() else 0\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"  - {f.name}/\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi export reranker model: {e}\")\n",
    "    print(\"\\nThử phương pháp thay thế với optimum-cli...\")\n",
    "    !optimum-cli export onnx --model {CFG.reranker_model_id} --task text-classification --trust-remote-code {CFG.reranker_output_dir}/fp32/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99226fc5",
   "metadata": {},
   "source": [
    "## 7. Quantize Reranker Model (INT8)\n",
    "\n",
    "Áp dụng Dynamic Quantization để giảm kích thước model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.quantize:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RERANKER MODEL - QUANTIZATION (INT8)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    reranker_fp32_path = CFG.reranker_output_dir / \"fp32\"\n",
    "    reranker_int8_path = CFG.reranker_output_dir / \"int8\"\n",
    "    reranker_int8_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Tải ONNX model để quantize\n",
    "        quantizer = ORTQuantizer.from_pretrained(reranker_fp32_path)\n",
    "\n",
    "        # Cấu hình quantization - dynamic quantization\n",
    "        qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "        # Áp dụng quantization\n",
    "        quantizer.quantize(save_dir=reranker_int8_path, quantization_config=qconfig)\n",
    "\n",
    "        # Copy các file tokenizer\n",
    "        for f in reranker_fp32_path.iterdir():\n",
    "            if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "                if f.is_file():\n",
    "                    shutil.copy(f, reranker_int8_path / f.name)\n",
    "\n",
    "        print(f\"\\nModel đã quantize được lưu tại: {reranker_int8_path}\")\n",
    "\n",
    "        # So sánh kích thước\n",
    "        fp32_size = sum(f.stat().st_size for f in reranker_fp32_path.glob(\"*.onnx\")) / (\n",
    "            1024**2\n",
    "        )\n",
    "        int8_size = sum(f.stat().st_size for f in reranker_int8_path.glob(\"*.onnx\")) / (\n",
    "            1024**2\n",
    "        )\n",
    "\n",
    "        print(f\"\\nSo sánh kích thước Model:\")\n",
    "        print(f\"  FP32: {fp32_size:.2f} MB\")\n",
    "        print(f\"  INT8: {int8_size:.2f} MB\")\n",
    "        print(f\"  Giảm: {(1 - int8_size / fp32_size) * 100:.1f}%\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi quantize reranker model: {e}\")\n",
    "        print(\"Bỏ qua quantization cho reranker model.\")\n",
    "else:\n",
    "    print(\"Quantization đã tắt. Bỏ qua...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2250e956",
   "metadata": {},
   "source": [
    "## 8. Kiểm Tra Models ONNX\n",
    "\n",
    "Kiểm tra các model ONNX đã export có hoạt động đúng không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49f83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"KIỂM TRA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kiểm tra Embedding Model\n",
    "print(\"\\n--- Kiểm tra Embedding Model (FP32) ---\")\n",
    "try:\n",
    "    embedding_fp32_path = CFG.embedding_output_dir / \"fp32\"\n",
    "\n",
    "    # Tải model\n",
    "    ort_embedding = ORTModelForFeatureExtraction.from_pretrained(embedding_fp32_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_fp32_path)\n",
    "\n",
    "    # Kiểm tra inference\n",
    "    test_text = \"Đây là câu kiểm tra cho embedding model.\"\n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = ort_embedding(**inputs)\n",
    "\n",
    "    print(f\"Input: {test_text}\")\n",
    "    print(f\"Output shape: {outputs.last_hidden_state.shape}\")\n",
    "    print(\"Embedding Model FP32: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"Embedding Model FP32 Lỗi: {e}\")\n",
    "\n",
    "# Kiểm tra Embedding Model INT8\n",
    "if CFG.quantize:\n",
    "    print(\"\\n--- Kiểm tra Embedding Model (INT8) ---\")\n",
    "    try:\n",
    "        embedding_int8_path = CFG.embedding_output_dir / \"int8\"\n",
    "\n",
    "        ort_embedding_int8 = ORTModelForFeatureExtraction.from_pretrained(\n",
    "            embedding_int8_path\n",
    "        )\n",
    "        outputs_int8 = ort_embedding_int8(**inputs)\n",
    "\n",
    "        print(f\"Output shape: {outputs_int8.last_hidden_state.shape}\")\n",
    "        print(\"Embedding Model INT8: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding Model INT8 Lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra Reranker Model\n",
    "print(\"\\n--- Kiểm tra Reranker Model (FP32) ---\")\n",
    "try:\n",
    "    reranker_fp32_path = CFG.reranker_output_dir / \"fp32\"\n",
    "\n",
    "    # Tải model\n",
    "    ort_reranker = ORTModelForSequenceClassification.from_pretrained(reranker_fp32_path)\n",
    "    tokenizer_reranker = AutoTokenizer.from_pretrained(reranker_fp32_path)\n",
    "\n",
    "    # Kiểm tra inference - Reranker nhận query và document\n",
    "    query = \"Machine learning là gì?\"\n",
    "    document = \"Machine learning là một nhánh của trí tuệ nhân tạo cho phép hệ thống học từ dữ liệu.\"\n",
    "\n",
    "    inputs_reranker = tokenizer_reranker(\n",
    "        query, document, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "    outputs_reranker = ort_reranker(**inputs_reranker)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Document: {document[:50]}...\")\n",
    "    print(f\"Score: {outputs_reranker.logits}\")\n",
    "    print(\"Reranker Model FP32: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"Reranker Model FP32 Lỗi: {e}\")\n",
    "\n",
    "# Kiểm tra Reranker Model INT8\n",
    "if CFG.quantize:\n",
    "    print(\"\\n--- Kiểm tra Reranker Model (INT8) ---\")\n",
    "    try:\n",
    "        reranker_int8_path = CFG.reranker_output_dir / \"int8\"\n",
    "\n",
    "        ort_reranker_int8 = ORTModelForSequenceClassification.from_pretrained(\n",
    "            reranker_int8_path\n",
    "        )\n",
    "        outputs_reranker_int8 = ort_reranker_int8(**inputs_reranker)\n",
    "\n",
    "        print(f\"Score: {outputs_reranker_int8.logits}\")\n",
    "        print(\"Reranker Model INT8: OK\")\n",
    "    except Exception as e:\n",
    "        print(f\"Reranker Model INT8 Lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d3c229",
   "metadata": {},
   "source": [
    "## 9. Tạo Model Card\n",
    "\n",
    "Tạo file README.md (Model Card) cho mỗi model để upload lên HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0027e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_card(model_name, source_model, model_type, hf_username):\n",
    "    \"\"\"Tạo nội dung Model Card cho HuggingFace Hub\"\"\"\n",
    "\n",
    "    task_name = (\n",
    "        \"feature-extraction\"\n",
    "        if model_type == \"embedding\"\n",
    "        else \"text-classification (reranking)\"\n",
    "    )\n",
    "    ort_class = (\n",
    "        \"FeatureExtraction\" if model_type == \"embedding\" else \"SequenceClassification\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "  - en\n",
    "  - zh\n",
    "tags:\n",
    "  - onnx\n",
    "  - optimum\n",
    "  - {model_type}\n",
    "  - qwen3\n",
    "  - sentence-similarity\n",
    "base_model: {source_model}\n",
    "library_name: optimum\n",
    "pipeline_tag: {\"feature-extraction\" if model_type == \"embedding\" else \"text-classification\"}\n",
    "---\n",
    "\n",
    "# {model_name}\n",
    "\n",
    "Phiên bản ONNX của [{source_model}](https://huggingface.co/{source_model})\n",
    "\n",
    "## Thông Tin Model\n",
    "\n",
    "| Thuộc tính | Giá trị |\n",
    "|------------|---------|  \n",
    "| Model nguồn | [{source_model}](https://huggingface.co/{source_model}) |\n",
    "| Định dạng | ONNX |\n",
    "| Quantization | FP32 + INT8 Dynamic |\n",
    "| Task | {task_name} |\n",
    "\n",
    "## Các Phiên Bản\n",
    "\n",
    "- `fp32/`: Model full precision (FP32)\n",
    "- `int8/`: Model đã quantize INT8 (dynamic quantization)\n",
    "\n",
    "## Cách Sử Dụng\n",
    "\n",
    "### Với Optimum\n",
    "\n",
    "```python\n",
    "from optimum.onnxruntime import ORTModelFor{ort_class}\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tải model FP32\n",
    "model = ORTModelFor{ort_class}.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\", \n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "\n",
    "# Hoặc tải model INT8 để inference nhanh hơn\n",
    "model_int8 = ORTModelFor{ort_class}.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"int8\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Với ONNX Runtime trực tiếp\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Tải tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\", \n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "\n",
    "# Tạo ONNX Runtime session\n",
    "session = ort.InferenceSession(\"fp32/model.onnx\")\n",
    "\n",
    "# Inference\n",
    "text = \"Your text here\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "outputs = session.run(None, dict(inputs))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Model này được phát hành theo giấy phép Apache 2.0, tuân theo giấy phép của model gốc.\n",
    "\n",
    "## Ghi Công\n",
    "\n",
    "- Model gốc: [Qwen Team](https://huggingface.co/Qwen)\n",
    "- Chuyển đổi ONNX: Sử dụng [Optimum](https://huggingface.co/docs/optimum)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd443bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TẠO MODEL CARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tạo Model Card cho Embedding Model\n",
    "print(\"\\n--- Tạo Model Card cho Embedding Model ---\")\n",
    "embedding_readme = create_model_card(\n",
    "    CFG.embedding_repo_name, CFG.embedding_model_id, \"embedding\", CFG.hf_username\n",
    ")\n",
    "embedding_readme_path = CFG.embedding_output_dir / \"README.md\"\n",
    "embedding_readme_path.write_text(embedding_readme, encoding=\"utf-8\")\n",
    "print(f\"Đã tạo: {embedding_readme_path}\")\n",
    "\n",
    "# Tạo Model Card cho Reranker Model\n",
    "print(\"\\n--- Tạo Model Card cho Reranker Model ---\")\n",
    "reranker_readme = create_model_card(\n",
    "    CFG.reranker_repo_name, CFG.reranker_model_id, \"reranker\", CFG.hf_username\n",
    ")\n",
    "reranker_readme_path = CFG.reranker_output_dir / \"README.md\"\n",
    "reranker_readme_path.write_text(reranker_readme, encoding=\"utf-8\")\n",
    "print(f\"Đã tạo: {reranker_readme_path}\")\n",
    "\n",
    "print(\"\\nModel Cards đã được tạo thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b634d0",
   "metadata": {},
   "source": [
    "## 10. Hướng Dẫn Upload Thủ Công Lên HuggingFace Hub\n",
    "\n",
    "Hướng dẫn từng bước để upload models lên HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HƯỚNG DẪN UPLOAD THỦ CÔNG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "Có 2 cách để upload models lên HuggingFace Hub:\n",
    "\n",
    "=== CÁCH 1: Sử dụng HuggingFace CLI ===\n",
    "\n",
    "1. Đăng nhập HuggingFace (chạy trong terminal):\n",
    "   $ huggingface-cli login\n",
    "\n",
    "2. Tạo repository trên HuggingFace:\n",
    "   $ huggingface-cli repo create {embedding_repo} --type model\n",
    "   $ huggingface-cli repo create {reranker_repo} --type model\n",
    "\n",
    "3. Upload Embedding Model:\n",
    "   $ huggingface-cli upload {hf_user}/{embedding_repo} {embedding_dir} .\n",
    "\n",
    "4. Upload Reranker Model:\n",
    "   $ huggingface-cli upload {hf_user}/{reranker_repo} {reranker_dir} .\n",
    "\n",
    "=== CÁCH 2: Sử dụng Web UI ===\n",
    "\n",
    "1. Truy cập: https://huggingface.co/new\n",
    "2. Tạo model repository mới\n",
    "3. Click \"Add file\" -> \"Upload files\"\n",
    "4. Upload toàn bộ thư mục model\n",
    "\n",
    "=== CẤU TRÚC THƯ MỤC CẦN UPLOAD ===\n",
    "\"\"\".format(\n",
    "        hf_user=CFG.hf_username,\n",
    "        embedding_repo=CFG.embedding_repo_name,\n",
    "        reranker_repo=CFG.reranker_repo_name,\n",
    "        embedding_dir=CFG.embedding_output_dir,\n",
    "        reranker_dir=CFG.reranker_output_dir,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hiển thị cấu trúc thư mục\n",
    "print(f\"\\n{CFG.embedding_repo_name}/\")\n",
    "for item in sorted(CFG.embedding_output_dir.rglob(\"*\")):\n",
    "    rel_path = item.relative_to(CFG.embedding_output_dir)\n",
    "    indent = \"  \" * len(rel_path.parts)\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"{indent}{item.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{indent}{item.name}/\")\n",
    "\n",
    "print(f\"\\n{CFG.reranker_repo_name}/\")\n",
    "for item in sorted(CFG.reranker_output_dir.rglob(\"*\")):\n",
    "    rel_path = item.relative_to(CFG.reranker_output_dir)\n",
    "    indent = \"  \" * len(rel_path.parts)\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"{indent}{item.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{indent}{item.name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9900f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra các lệnh CLI cụ thể để copy-paste\n",
    "print(\"=\" * 60)\n",
    "print(\"LỆNH CLI ĐỂ UPLOAD (copy và chạy trong terminal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "commands = f\"\"\"\n",
    "# 1. Đăng nhập HuggingFace\n",
    "huggingface-cli login\n",
    "\n",
    "# 2. Tạo repositories\n",
    "huggingface-cli repo create {CFG.embedding_repo_name} --type model\n",
    "huggingface-cli repo create {CFG.reranker_repo_name} --type model\n",
    "\n",
    "# 3. Upload Embedding Model\n",
    "huggingface-cli upload {CFG.hf_username}/{CFG.embedding_repo_name} \"{CFG.embedding_output_dir}\" .\n",
    "\n",
    "# 4. Upload Reranker Model  \n",
    "huggingface-cli upload {CFG.hf_username}/{CFG.reranker_repo_name} \"{CFG.reranker_output_dir}\" .\n",
    "\"\"\"\n",
    "\n",
    "print(commands)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAU KHI UPLOAD, MODELS SẼ CÓ TẠI:\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"  - Embedding: https://huggingface.co/{CFG.hf_username}/{CFG.embedding_repo_name}\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Reranker: https://huggingface.co/{CFG.hf_username}/{CFG.reranker_repo_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8a25f",
   "metadata": {},
   "source": [
    "## 11. Tổng Kết\n",
    "\n",
    "Tổng kết quá trình chuyển đổi và các bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28e4e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TỔNG KẾT CHUYỂN ĐỔI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModels nguồn:\")\n",
    "print(f\"  - Embedding: {CFG.embedding_model_id}\")\n",
    "print(f\"  - Reranker: {CFG.reranker_model_id}\")\n",
    "\n",
    "print(\"\\nModels đã chuyển đổi:\")\n",
    "print(f\"  - Embedding ONNX: {CFG.embedding_output_dir}\")\n",
    "print(f\"  - Reranker ONNX: {CFG.reranker_output_dir}\")\n",
    "\n",
    "\n",
    "# Tính tổng kích thước\n",
    "def get_folder_size(folder):\n",
    "    return sum(f.stat().st_size for f in Path(folder).rglob(\"*\") if f.is_file()) / (\n",
    "        1024**2\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\nKích thước:\")\n",
    "try:\n",
    "    emb_size = get_folder_size(CFG.embedding_output_dir)\n",
    "    rer_size = get_folder_size(CFG.reranker_output_dir)\n",
    "    print(f\"  - Embedding: {emb_size:.2f} MB\")\n",
    "    print(f\"  - Reranker: {rer_size:.2f} MB\")\n",
    "    print(f\"  - Tổng cộng: {emb_size + rer_size:.2f} MB\")\n",
    "except:\n",
    "    print(\"  - Không thể tính kích thước\")\n",
    "\n",
    "print(\"\\nCác phiên bản có sẵn:\")\n",
    "print(\"  - FP32 (Full Precision)\")\n",
    "if CFG.quantize:\n",
    "    print(\"  - INT8 (Dynamic Quantization)\")\n",
    "\n",
    "print(\"\\nBước tiếp theo:\")\n",
    "print(\"  1. Chạy các lệnh CLI ở Section 10 để upload lên HuggingFace Hub\")\n",
    "print(\"  2. Kiểm tra repositories trên HuggingFace\")\n",
    "print(\"  3. Test models từ HuggingFace Hub\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOÀN THÀNH!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-competitions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
