{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "191c078e",
   "metadata": {},
   "source": [
    "# Qwen3 Embedding & Reranker - Chuyển Đổi ONNX và Quantization\n",
    "\n",
    "**Mô tả:**\n",
    "- Tải xuống models từ HuggingFace\n",
    "- Chuyển đổi sang định dạng ONNX\n",
    "- Quantize (INT8 Dynamic) để giảm kích thước\n",
    "- Tạo Model Card và hướng dẫn upload thủ công\n",
    "\n",
    "**Models:**\n",
    "- `Qwen/Qwen3-Embedding-0.6B` - Model embedding cho semantic search\n",
    "- `Qwen/Qwen3-Reranker-0.6B` - Model reranker cho document reranking\n",
    "\n",
    "**Lưu ý về RAM:**\n",
    "- Notebook này cần khoảng 8-12GB RAM\n",
    "- Trên Colab Free (12.7GB RAM): Có thể chạy được nhưng cần restart runtime giữa 2 models\n",
    "- Quantization chạy trên CPU, không tận dụng được GPU\n",
    "- Nếu vẫn tràn RAM: Chạy từng model riêng biệt với restart runtime giữa mỗi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe935bf",
   "metadata": {},
   "source": [
    "## 1. Bootstrap - Thiết Lập Môi Trường\n",
    "\n",
    "Cell này tự động phát hiện và cấu hình môi trường (local/colab/kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522d9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BOOTSTRAP CELL - UNIVERSAL SETUP ===\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Cấu hình GitHub\n",
    "GITHUB_USER = \"n24q02m\"\n",
    "REPO_NAME = \"n24q02m-kaggle-competitions\"\n",
    "BRANCH = \"main\"\n",
    "\n",
    "\n",
    "# Phát hiện môi trường\n",
    "def detect_env():\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        return \"colab\"\n",
    "    elif \"kaggle_web_client\" in sys.modules or os.path.exists(\"/kaggle\"):\n",
    "        return \"kaggle\"\n",
    "    else:\n",
    "        return \"local\"\n",
    "\n",
    "\n",
    "ENV = detect_env()\n",
    "print(f\"Phát hiện môi trường: {ENV.upper()}\")\n",
    "\n",
    "# Thiết lập theo môi trường\n",
    "if ENV == \"local\":\n",
    "    # Local: Import trực tiếp từ repo\n",
    "    repo_root = Path.cwd().parent\n",
    "    if str(repo_root) not in sys.path:\n",
    "        sys.path.insert(0, str(repo_root))\n",
    "\n",
    "    from core import setup_env\n",
    "\n",
    "    env = setup_env.setup()\n",
    "\n",
    "else:\n",
    "    # Cloud: Tải setup_env.py từ GitHub\n",
    "    import requests\n",
    "    import subprocess\n",
    "\n",
    "    CORE_URL = (\n",
    "        f\"https://raw.githubusercontent.com/{GITHUB_USER}/{REPO_NAME}/{BRANCH}/core\"\n",
    "    )\n",
    "\n",
    "    # Tải setup_env.py\n",
    "    print(\"Đang tải setup_env.py...\")\n",
    "    response = requests.get(f\"{CORE_URL}/setup_env.py\")\n",
    "    with open(\"setup_env.py\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    # Import và thiết lập\n",
    "    import setup_env\n",
    "\n",
    "    env = setup_env.setup(GITHUB_USER, REPO_NAME)\n",
    "\n",
    "# Hiển thị thông tin môi trường\n",
    "env.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d5dc9c",
   "metadata": {},
   "source": [
    "## 2. Cấu Hình\n",
    "\n",
    "Cấu hình các model và đường dẫn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ca18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    # Model nguồn từ HuggingFace\n",
    "    embedding_model_id = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "    reranker_model_id = \"Qwen/Qwen3-Reranker-0.6B\"\n",
    "\n",
    "    # HuggingFace username (thay đổi theo username của bạn)\n",
    "    hf_username = \"n24q02m\"\n",
    "\n",
    "    # Tên repo đích trên HuggingFace Hub\n",
    "    embedding_repo_name = \"Qwen3-Embedding-0.6B-ONNX\"\n",
    "    reranker_repo_name = \"Qwen3-Reranker-0.6B-ONNX\"\n",
    "\n",
    "    # Thư mục output\n",
    "    output_dir = Path.cwd() / \"models\"\n",
    "    embedding_output_dir = output_dir / \"qwen3-embedding-onnx\"\n",
    "    reranker_output_dir = output_dir / \"qwen3-reranker-onnx\"\n",
    "\n",
    "    # Cài đặt ONNX Export\n",
    "    opset_version = 14\n",
    "\n",
    "    # Cài đặt Quantization\n",
    "    quantize = True\n",
    "    quantization_type = \"dynamic\"  # dynamic hoặc static\n",
    "\n",
    "\n",
    "# Tạo thư mục output\n",
    "CFG.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "CFG.embedding_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "CFG.reranker_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Cấu hình:\")\n",
    "print(f\"  - Embedding Model: {CFG.embedding_model_id}\")\n",
    "print(f\"  - Reranker Model: {CFG.reranker_model_id}\")\n",
    "print(f\"  - Thư mục Output: {CFG.output_dir}\")\n",
    "print(f\"  - Quantization: {CFG.quantize} ({CFG.quantization_type})\")\n",
    "print(f\"  - HuggingFace Username: {CFG.hf_username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e754177b",
   "metadata": {},
   "source": [
    "## 3. Import Thư Viện\n",
    "\n",
    "Import các thư viện cần thiết (đã được cài đặt từ requirements.txt hoặc sẵn có trên Colab/Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fb573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện\n",
    "import gc\n",
    "import torch\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import shutil\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import (\n",
    "    ORTModelForFeatureExtraction,\n",
    "    ORTModelForSequenceClassification,\n",
    ")\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Giải phóng bộ nhớ RAM và GPU\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    print(\"Đã giải phóng bộ nhớ.\")\n",
    "\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ONNX version: {onnx.__version__}\")\n",
    "print(f\"ONNX Runtime version: {ort.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764d3e0",
   "metadata": {},
   "source": [
    "## 4. Tải Xuống và Export Embedding Model sang ONNX\n",
    "\n",
    "Tải xuống Qwen3-Embedding-0.6B từ HuggingFace và chuyển đổi sang định dạng ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4893ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDING MODEL - CHUYỂN ĐỔI ONNX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nĐang tải model: {CFG.embedding_model_id}\")\n",
    "print(\"Quá trình này có thể mất vài phút...\")\n",
    "\n",
    "embedding_onnx_path = CFG.embedding_output_dir / \"fp32\"\n",
    "embedding_onnx_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer\n",
    "    embedding_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CFG.embedding_model_id, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Export sang ONNX sử dụng optimum\n",
    "    embedding_ort_model = ORTModelForFeatureExtraction.from_pretrained(\n",
    "        CFG.embedding_model_id, export=True, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Lưu model và tokenizer\n",
    "    embedding_ort_model.save_pretrained(embedding_onnx_path)\n",
    "    embedding_tokenizer.save_pretrained(embedding_onnx_path)\n",
    "\n",
    "    print(f\"\\nEmbedding model đã được export tại: {embedding_onnx_path}\")\n",
    "    print(\"Danh sách files:\")\n",
    "    for f in embedding_onnx_path.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024**2) if f.is_file() else 0\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"  - {f.name}/\")\n",
    "\n",
    "    # Giải phóng bộ nhớ ngay sau khi lưu\n",
    "    del embedding_ort_model\n",
    "    del embedding_tokenizer\n",
    "    clear_memory()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi export embedding model: {e}\")\n",
    "    print(\"\\nThử phương pháp thay thế với optimum-cli...\")\n",
    "    !optimum-cli export onnx --model {CFG.embedding_model_id} --task feature-extraction --trust-remote-code {embedding_onnx_path}\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c684dcd8",
   "metadata": {},
   "source": [
    "## 5. Quantize Embedding Model (INT8)\n",
    "\n",
    "Áp dụng Dynamic Quantization để giảm kích thước model.\n",
    "\n",
    "**Lưu ý:** Quantization trong ONNX Runtime chạy trên CPU, không tận dụng được GPU. Nếu Python API thất bại sẽ tự động fallback sang CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09bf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.quantize:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EMBEDDING MODEL - QUANTIZATION (INT8)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    embedding_fp32_path = CFG.embedding_output_dir / \"fp32\"\n",
    "    embedding_int8_path = CFG.embedding_output_dir / \"int8\"\n",
    "    embedding_int8_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        # Quantization trực tiếp trong Python\n",
    "        quantizer = ORTQuantizer.from_pretrained(embedding_fp32_path)\n",
    "        qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "        quantizer.quantize(save_dir=embedding_int8_path, quantization_config=qconfig)\n",
    "\n",
    "        # Copy tokenizer files\n",
    "        for f in embedding_fp32_path.iterdir():\n",
    "            if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "                if f.is_file():\n",
    "                    shutil.copy(f, embedding_int8_path / f.name)\n",
    "\n",
    "        print(f\"\\nModel đã quantize được lưu tại: {embedding_int8_path}\")\n",
    "\n",
    "        # So sánh kích thước\n",
    "        fp32_size = sum(\n",
    "            f.stat().st_size for f in embedding_fp32_path.glob(\"*.onnx\")\n",
    "        ) / (1024**2)\n",
    "        int8_size = sum(\n",
    "            f.stat().st_size for f in embedding_int8_path.glob(\"*.onnx\")\n",
    "        ) / (1024**2)\n",
    "\n",
    "        print(f\"\\nSo sánh kích thước Model:\")\n",
    "        print(f\"  FP32: {fp32_size:.2f} MB\")\n",
    "        print(f\"  INT8: {int8_size:.2f} MB\")\n",
    "        print(f\"  Giảm: {(1 - int8_size / fp32_size) * 100:.1f}%\")\n",
    "\n",
    "        del quantizer\n",
    "        clear_memory()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi quantize embedding model: {e}\")\n",
    "        print(\"\\nThử phương pháp CLI...\")\n",
    "        !optimum-cli onnxruntime quantize --onnx_model \"{embedding_fp32_path}\" --avx512_vnni -o \"{embedding_int8_path}\"\n",
    "\n",
    "        # Copy tokenizer files\n",
    "        for f in embedding_fp32_path.iterdir():\n",
    "            if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "                if f.is_file() and not (embedding_int8_path / f.name).exists():\n",
    "                    shutil.copy(f, embedding_int8_path / f.name)\n",
    "        clear_memory()\n",
    "else:\n",
    "    print(\"Quantization đã tắt. Bỏ qua...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EMBEDDING MODEL - HOÀN THÀNH!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82338e0",
   "metadata": {},
   "source": [
    "## 6. Tải Xuống và Export Reranker Model sang ONNX\n",
    "\n",
    "Tải xuống Qwen3-Reranker-0.6B từ HuggingFace và chuyển đổi sang định dạng ONNX.\n",
    "\n",
    "**Trên Colab:** Nếu vẫn bị tràn RAM, hãy restart runtime (Runtime -> Restart runtime) trước khi chạy cell này. Sau đó chạy lại từ Section 1-3, bỏ qua Section 4-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RERANKER MODEL - CHUYỂN ĐỔI ONNX\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nĐang tải model: {CFG.reranker_model_id}\")\n",
    "print(\"Quá trình này có thể mất vài phút...\")\n",
    "\n",
    "reranker_onnx_path = CFG.reranker_output_dir / \"fp32\"\n",
    "reranker_onnx_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # Tải tokenizer\n",
    "    reranker_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        CFG.reranker_model_id, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Export sang ONNX - Reranker là cross-encoder, sử dụng SequenceClassification\n",
    "    reranker_ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "        CFG.reranker_model_id, export=True, trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # Lưu model và tokenizer\n",
    "    reranker_ort_model.save_pretrained(reranker_onnx_path)\n",
    "    reranker_tokenizer.save_pretrained(reranker_onnx_path)\n",
    "\n",
    "    print(f\"\\nReranker model đã được export tại: {reranker_onnx_path}\")\n",
    "    print(\"Danh sách files:\")\n",
    "    for f in reranker_onnx_path.iterdir():\n",
    "        size_mb = f.stat().st_size / (1024**2) if f.is_file() else 0\n",
    "        print(f\"  - {f.name} ({size_mb:.2f} MB)\" if size_mb > 0 else f\"  - {f.name}/\")\n",
    "\n",
    "    # Giải phóng bộ nhớ ngay sau khi lưu\n",
    "    del reranker_ort_model\n",
    "    del reranker_tokenizer\n",
    "    clear_memory()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Lỗi khi export reranker model: {e}\")\n",
    "    print(\"\\nThử phương pháp thay thế với optimum-cli...\")\n",
    "    !optimum-cli export onnx --model {CFG.reranker_model_id} --task text-classification --trust-remote-code {reranker_onnx_path}\n",
    "    clear_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97dedb8",
   "metadata": {},
   "source": [
    "## 7. Quantize Reranker Model (INT8)\n",
    "\n",
    "Áp dụng Dynamic Quantization để giảm kích thước model. Nếu Python API thất bại sẽ tự động fallback sang CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.quantize:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"RERANKER MODEL - QUANTIZATION (INT8)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    reranker_fp32_path = CFG.reranker_output_dir / \"fp32\"\n",
    "    reranker_int8_path = CFG.reranker_output_dir / \"int8\"\n",
    "    reranker_int8_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Sử dụng subprocess để chạy quantization - giải phóng RAM sau khi xong\n",
    "    quantize_script = f'''\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "fp32_path = Path(\"{reranker_fp32_path}\")\n",
    "int8_path = Path(\"{reranker_int8_path}\")\n",
    "\n",
    "# Quantize\n",
    "quantizer = ORTQuantizer.from_pretrained(fp32_path)\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "quantizer.quantize(save_dir=int8_path, quantization_config=qconfig)\n",
    "\n",
    "# Copy tokenizer files\n",
    "for f in fp32_path.iterdir():\n",
    "    if not f.name.endswith(\".onnx\") and not f.name.endswith(\"_data\"):\n",
    "        if f.is_file():\n",
    "            shutil.copy(f, int8_path / f.name)\n",
    "\n",
    "print(\"Quantization completed!\")\n",
    "'''\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"python\", \"-c\", quantize_script],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=600,  # 10 phút timeout\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0:\n",
    "            print(result.stdout)\n",
    "            print(f\"\\nModel đã quantize được lưu tại: {reranker_int8_path}\")\n",
    "\n",
    "            # So sánh kích thước\n",
    "            fp32_size = sum(\n",
    "                f.stat().st_size for f in reranker_fp32_path.glob(\"*.onnx\")\n",
    "            ) / (1024**2)\n",
    "            int8_size = sum(\n",
    "                f.stat().st_size for f in reranker_int8_path.glob(\"*.onnx\")\n",
    "            ) / (1024**2)\n",
    "\n",
    "            print(f\"\\nSo sánh kích thước Model:\")\n",
    "            print(f\"  FP32: {fp32_size:.2f} MB\")\n",
    "            print(f\"  INT8: {int8_size:.2f} MB\")\n",
    "            print(f\"  Giảm: {(1 - int8_size / fp32_size) * 100:.1f}%\")\n",
    "        else:\n",
    "            print(f\"Lỗi: {result.stderr}\")\n",
    "            raise Exception(result.stderr)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi khi quantize reranker model: {e}\")\n",
    "        print(\"Bỏ qua quantization cho reranker model.\")\n",
    "\n",
    "    clear_memory()\n",
    "else:\n",
    "    print(\"Quantization đã tắt. Bỏ qua...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RERANKER MODEL - HOÀN THÀNH!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ad6c5",
   "metadata": {},
   "source": [
    "## 8. Kiểm Tra Models ONNX\n",
    "\n",
    "Kiểm tra các model ONNX đã export có hoạt động đúng không"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26026bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"KIỂM TRA MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Kiểm tra Embedding Model\n",
    "print(\"\\n--- Kiểm tra Embedding Model (FP32) ---\")\n",
    "try:\n",
    "    embedding_fp32_path = CFG.embedding_output_dir / \"fp32\"\n",
    "\n",
    "    # Tải model\n",
    "    ort_embedding = ORTModelForFeatureExtraction.from_pretrained(embedding_fp32_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_fp32_path)\n",
    "\n",
    "    # Kiểm tra inference\n",
    "    test_text = \"Đây là câu kiểm tra cho embedding model.\"\n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = ort_embedding(**inputs)\n",
    "\n",
    "    print(f\"Input: {test_text}\")\n",
    "    print(f\"Output shape: {outputs.last_hidden_state.shape}\")\n",
    "    print(\"Embedding Model FP32: OK\")\n",
    "\n",
    "    # Giải phóng\n",
    "    del ort_embedding, tokenizer, inputs, outputs\n",
    "    clear_memory()\n",
    "except Exception as e:\n",
    "    print(f\"Embedding Model FP32 Lỗi: {e}\")\n",
    "\n",
    "# Kiểm tra Embedding Model INT8\n",
    "if CFG.quantize:\n",
    "    print(\"\\n--- Kiểm tra Embedding Model (INT8) ---\")\n",
    "    try:\n",
    "        embedding_int8_path = CFG.embedding_output_dir / \"int8\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(embedding_int8_path)\n",
    "        ort_embedding_int8 = ORTModelForFeatureExtraction.from_pretrained(\n",
    "            embedding_int8_path\n",
    "        )\n",
    "\n",
    "        test_text = \"Đây là câu kiểm tra cho embedding model.\"\n",
    "        inputs = tokenizer(\n",
    "            test_text, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        outputs_int8 = ort_embedding_int8(**inputs)\n",
    "\n",
    "        print(f\"Output shape: {outputs_int8.last_hidden_state.shape}\")\n",
    "        print(\"Embedding Model INT8: OK\")\n",
    "\n",
    "        del ort_embedding_int8, tokenizer, inputs, outputs_int8\n",
    "        clear_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding Model INT8 Lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc1cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiểm tra Reranker Model\n",
    "print(\"\\n--- Kiểm tra Reranker Model (FP32) ---\")\n",
    "try:\n",
    "    reranker_fp32_path = CFG.reranker_output_dir / \"fp32\"\n",
    "\n",
    "    # Tải model\n",
    "    ort_reranker = ORTModelForSequenceClassification.from_pretrained(reranker_fp32_path)\n",
    "    tokenizer_reranker = AutoTokenizer.from_pretrained(reranker_fp32_path)\n",
    "\n",
    "    # Kiểm tra inference - Reranker nhận query và document\n",
    "    query = \"Machine learning là gì?\"\n",
    "    document = \"Machine learning là một nhánh của trí tuệ nhân tạo cho phép hệ thống học từ dữ liệu.\"\n",
    "\n",
    "    inputs_reranker = tokenizer_reranker(\n",
    "        query, document, return_tensors=\"pt\", padding=True, truncation=True\n",
    "    )\n",
    "    outputs_reranker = ort_reranker(**inputs_reranker)\n",
    "\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Document: {document[:50]}...\")\n",
    "    print(f\"Score: {outputs_reranker.logits}\")\n",
    "    print(\"Reranker Model FP32: OK\")\n",
    "\n",
    "    del ort_reranker, tokenizer_reranker, inputs_reranker, outputs_reranker\n",
    "    clear_memory()\n",
    "except Exception as e:\n",
    "    print(f\"Reranker Model FP32 Lỗi: {e}\")\n",
    "\n",
    "# Kiểm tra Reranker Model INT8\n",
    "if CFG.quantize:\n",
    "    print(\"\\n--- Kiểm tra Reranker Model (INT8) ---\")\n",
    "    try:\n",
    "        reranker_int8_path = CFG.reranker_output_dir / \"int8\"\n",
    "\n",
    "        ort_reranker_int8 = ORTModelForSequenceClassification.from_pretrained(\n",
    "            reranker_int8_path\n",
    "        )\n",
    "        tokenizer_reranker = AutoTokenizer.from_pretrained(reranker_int8_path)\n",
    "\n",
    "        query = \"Machine learning là gì?\"\n",
    "        document = \"Machine learning là một nhánh của trí tuệ nhân tạo.\"\n",
    "        inputs_reranker = tokenizer_reranker(\n",
    "            query, document, return_tensors=\"pt\", padding=True, truncation=True\n",
    "        )\n",
    "        outputs_reranker_int8 = ort_reranker_int8(**inputs_reranker)\n",
    "\n",
    "        print(f\"Score: {outputs_reranker_int8.logits}\")\n",
    "        print(\"Reranker Model INT8: OK\")\n",
    "\n",
    "        del (\n",
    "            ort_reranker_int8,\n",
    "            tokenizer_reranker,\n",
    "            inputs_reranker,\n",
    "            outputs_reranker_int8,\n",
    "        )\n",
    "        clear_memory()\n",
    "    except Exception as e:\n",
    "        print(f\"Reranker Model INT8 Lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd4cf60",
   "metadata": {},
   "source": [
    "## 9. Tạo Model Card\n",
    "\n",
    "Tạo file README.md (Model Card) cho mỗi model để upload lên HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c2f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_card(model_name, source_model, model_type, hf_username):\n",
    "    \"\"\"Tạo nội dung Model Card cho HuggingFace Hub\"\"\"\n",
    "\n",
    "    task_name = (\n",
    "        \"feature-extraction\"\n",
    "        if model_type == \"embedding\"\n",
    "        else \"text-classification (reranking)\"\n",
    "    )\n",
    "    ort_class = (\n",
    "        \"FeatureExtraction\" if model_type == \"embedding\" else \"SequenceClassification\"\n",
    "    )\n",
    "\n",
    "    return f\"\"\"---\n",
    "license: apache-2.0\n",
    "language:\n",
    "  - en\n",
    "  - zh\n",
    "tags:\n",
    "  - onnx\n",
    "  - optimum\n",
    "  - {model_type}\n",
    "  - qwen3\n",
    "  - sentence-similarity\n",
    "base_model: {source_model}\n",
    "library_name: optimum\n",
    "pipeline_tag: {\"feature-extraction\" if model_type == \"embedding\" else \"text-classification\"}\n",
    "---\n",
    "\n",
    "# {model_name}\n",
    "\n",
    "Phiên bản ONNX của [{source_model}](https://huggingface.co/{source_model})\n",
    "\n",
    "## Thông Tin Model\n",
    "\n",
    "| Thuộc tính | Giá trị |\n",
    "|------------|---------|\n",
    "| Model nguồn | [{source_model}](https://huggingface.co/{source_model}) |\n",
    "| Định dạng | ONNX |\n",
    "| Quantization | FP32 + INT8 Dynamic |\n",
    "| Task | {task_name} |\n",
    "\n",
    "## Các Phiên Bản\n",
    "\n",
    "- `fp32/`: Model full precision (FP32)\n",
    "- `int8/`: Model đã quantize INT8 (dynamic quantization)\n",
    "\n",
    "## Cách Sử Dụng\n",
    "\n",
    "### Với Optimum\n",
    "\n",
    "```python\n",
    "from optimum.onnxruntime import ORTModelFor{ort_class}\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Tải model FP32\n",
    "model = ORTModelFor{ort_class}.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "\n",
    "# Hoặc tải model INT8 để inference nhanh hơn\n",
    "model_int8 = ORTModelFor{ort_class}.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"int8\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Với ONNX Runtime trực tiếp\n",
    "\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Tải tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"{hf_username}/{model_name}\",\n",
    "    subfolder=\"fp32\"\n",
    ")\n",
    "\n",
    "# Tạo ONNX Runtime session\n",
    "session = ort.InferenceSession(\"fp32/model.onnx\")\n",
    "\n",
    "# Inference\n",
    "text = \"Your text here\"\n",
    "inputs = tokenizer(text, return_tensors=\"np\")\n",
    "outputs = session.run(None, dict(inputs))\n",
    "```\n",
    "\n",
    "## License\n",
    "\n",
    "Model này được phát hành theo giấy phép Apache 2.0, tuân theo giấy phép của model gốc.\n",
    "\n",
    "## Ghi Công\n",
    "\n",
    "- Model gốc: [Qwen Team](https://huggingface.co/Qwen)\n",
    "- Chuyển đổi ONNX: Sử dụng [Optimum](https://huggingface.co/docs/optimum)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00c4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TẠO MODEL CARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Tạo Model Card cho Embedding Model\n",
    "print(\"\\n--- Tạo Model Card cho Embedding Model ---\")\n",
    "embedding_readme = create_model_card(\n",
    "    CFG.embedding_repo_name, CFG.embedding_model_id, \"embedding\", CFG.hf_username\n",
    ")\n",
    "embedding_readme_path = CFG.embedding_output_dir / \"README.md\"\n",
    "embedding_readme_path.write_text(embedding_readme, encoding=\"utf-8\")\n",
    "print(f\"Đã tạo: {embedding_readme_path}\")\n",
    "\n",
    "# Tạo Model Card cho Reranker Model\n",
    "print(\"\\n--- Tạo Model Card cho Reranker Model ---\")\n",
    "reranker_readme = create_model_card(\n",
    "    CFG.reranker_repo_name, CFG.reranker_model_id, \"reranker\", CFG.hf_username\n",
    ")\n",
    "reranker_readme_path = CFG.reranker_output_dir / \"README.md\"\n",
    "reranker_readme_path.write_text(reranker_readme, encoding=\"utf-8\")\n",
    "print(f\"Đã tạo: {reranker_readme_path}\")\n",
    "\n",
    "print(\"\\nModel Cards đã được tạo thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a43dc",
   "metadata": {},
   "source": [
    "## 10. Hướng Dẫn Upload Thủ Công Lên HuggingFace Hub\n",
    "\n",
    "Hướng dẫn từng bước để upload models lên HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HƯỚNG DẪN UPLOAD THỦ CÔNG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\n",
    "    \"\"\"\n",
    "Có 2 cách để upload models lên HuggingFace Hub:\n",
    "\n",
    "=== CÁCH 1: Sử dụng HuggingFace CLI ===\n",
    "\n",
    "1. Đăng nhập HuggingFace (chạy trong terminal):\n",
    "   $ huggingface-cli login\n",
    "\n",
    "2. Tạo repository trên HuggingFace:\n",
    "   $ huggingface-cli repo create {embedding_repo} --type model\n",
    "   $ huggingface-cli repo create {reranker_repo} --type model\n",
    "\n",
    "3. Upload Embedding Model:\n",
    "   $ huggingface-cli upload {hf_user}/{embedding_repo} {embedding_dir} .\n",
    "\n",
    "4. Upload Reranker Model:\n",
    "   $ huggingface-cli upload {hf_user}/{reranker_repo} {reranker_dir} .\n",
    "\n",
    "=== CÁCH 2: Sử dụng Web UI ===\n",
    "\n",
    "1. Truy cập: https://huggingface.co/new\n",
    "2. Tạo model repository mới\n",
    "3. Click \"Add file\" -> \"Upload files\"\n",
    "4. Upload toàn bộ thư mục model\n",
    "\n",
    "=== CẤU TRÚC THƯ MỤC CẦN UPLOAD ===\n",
    "\"\"\".format(\n",
    "        hf_user=CFG.hf_username,\n",
    "        embedding_repo=CFG.embedding_repo_name,\n",
    "        reranker_repo=CFG.reranker_repo_name,\n",
    "        embedding_dir=CFG.embedding_output_dir,\n",
    "        reranker_dir=CFG.reranker_output_dir,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Hiển thị cấu trúc thư mục\n",
    "print(f\"\\n{CFG.embedding_repo_name}/\")\n",
    "for item in sorted(CFG.embedding_output_dir.rglob(\"*\")):\n",
    "    rel_path = item.relative_to(CFG.embedding_output_dir)\n",
    "    indent = \"  \" * len(rel_path.parts)\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"{indent}{item.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{indent}{item.name}/\")\n",
    "\n",
    "print(f\"\\n{CFG.reranker_repo_name}/\")\n",
    "for item in sorted(CFG.reranker_output_dir.rglob(\"*\")):\n",
    "    rel_path = item.relative_to(CFG.reranker_output_dir)\n",
    "    indent = \"  \" * len(rel_path.parts)\n",
    "    if item.is_file():\n",
    "        size_mb = item.stat().st_size / (1024**2)\n",
    "        print(f\"{indent}{item.name} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"{indent}{item.name}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In ra các lệnh CLI cụ thể để copy-paste\n",
    "print(\"=\" * 60)\n",
    "print(\"LỆNH CLI ĐỂ UPLOAD (copy và chạy trong terminal)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "commands = f\"\"\"\n",
    "# 1. Đăng nhập HuggingFace\n",
    "huggingface-cli login\n",
    "\n",
    "# 2. Tạo repositories\n",
    "huggingface-cli repo create {CFG.embedding_repo_name} --type model\n",
    "huggingface-cli repo create {CFG.reranker_repo_name} --type model\n",
    "\n",
    "# 3. Upload Embedding Model\n",
    "huggingface-cli upload {CFG.hf_username}/{CFG.embedding_repo_name} \"{CFG.embedding_output_dir}\" .\n",
    "\n",
    "# 4. Upload Reranker Model\n",
    "huggingface-cli upload {CFG.hf_username}/{CFG.reranker_repo_name} \"{CFG.reranker_output_dir}\" .\n",
    "\"\"\"\n",
    "\n",
    "print(commands)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAU KHI UPLOAD, MODELS SẼ CÓ TẠI:\")\n",
    "print(\"=\" * 60)\n",
    "print(\n",
    "    f\"  - Embedding: https://huggingface.co/{CFG.hf_username}/{CFG.embedding_repo_name}\"\n",
    ")\n",
    "print(\n",
    "    f\"  - Reranker: https://huggingface.co/{CFG.hf_username}/{CFG.reranker_repo_name}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c2736",
   "metadata": {},
   "source": [
    "## 11. Tổng Kết\n",
    "\n",
    "Tổng kết quá trình chuyển đổi và các bước tiếp theo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e48f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TỔNG KẾT CHUYỂN ĐỔI\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModels nguồn:\")\n",
    "print(f\"  - Embedding: {CFG.embedding_model_id}\")\n",
    "print(f\"  - Reranker: {CFG.reranker_model_id}\")\n",
    "\n",
    "print(\"\\nModels đã chuyển đổi:\")\n",
    "print(f\"  - Embedding ONNX: {CFG.embedding_output_dir}\")\n",
    "print(f\"  - Reranker ONNX: {CFG.reranker_output_dir}\")\n",
    "\n",
    "\n",
    "# Tính tổng kích thước\n",
    "def get_folder_size(folder):\n",
    "    return sum(f.stat().st_size for f in Path(folder).rglob(\"*\") if f.is_file()) / (\n",
    "        1024**2\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\nKích thước:\")\n",
    "try:\n",
    "    emb_size = get_folder_size(CFG.embedding_output_dir)\n",
    "    rer_size = get_folder_size(CFG.reranker_output_dir)\n",
    "    print(f\"  - Embedding: {emb_size:.2f} MB\")\n",
    "    print(f\"  - Reranker: {rer_size:.2f} MB\")\n",
    "    print(f\"  - Tổng cộng: {emb_size + rer_size:.2f} MB\")\n",
    "except:\n",
    "    print(\"  - Không thể tính kích thước\")\n",
    "\n",
    "print(\"\\nCác phiên bản có sẵn:\")\n",
    "print(\"  - FP32 (Full Precision)\")\n",
    "if CFG.quantize:\n",
    "    print(\"  - INT8 (Dynamic Quantization)\")\n",
    "\n",
    "print(\"\\nBước tiếp theo:\")\n",
    "print(\"  1. Chạy các lệnh CLI ở Section 10 để upload lên HuggingFace Hub\")\n",
    "print(\"  2. Kiểm tra repositories trên HuggingFace\")\n",
    "print(\"  3. Test models từ HuggingFace Hub\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"HOÀN THÀNH!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
